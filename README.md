# finite-difference-policy-gradient-on-cartpole
Estimating CartPole policy gradients using Finite Differences

Philip Docena

This repository has sample code for a gradient-free stochastic optimization.  We do this via finite differences.  I wrote this code based on the policy search text on Scholarpedia (see http://www.scholarpedia.org/article/Policy_gradient_methods).  Note that my interpretation might be different from actual or typical finite difference implementations.

The Finite Differences (FD) method works the same way a standard gradient descent does.  In gradient descent (or ascent), the parameter changes are guided by the gradient.  Based on design intentions and constraints, this gradient is calculated either as a ‘global’ value by considering all available samples (batch gradient descent, slow calculations due to volume but a more direct convergence), by considering only one sample (stochastic gradient descent (SGD), fast calculations but slower convergence), or by using some number of samples in between (mini-batch GD).

Mathematically, the gradient is represented as a vector of partial derivatives calculated with respect to the parameters.  FD approximates these partial derivatives through the actual changes in the output value (the cumulative discounted rewards in the case of RL problems) due to small changes in the parameters.  Ideally, FD is calculated by individually perturbing each parameter by a small epsilon amount and observing the new reward.  This is done to each coefficient/parameter in the linear combination, including compounded, non-linear parameters.  Since the perturbation is applied only to one parameter each time, the increase (or decrease) in reward is induced along a specific parameter dimension in the policy reward space.

In the CartPole problem (and the nature of RL problems in general), it is impossible to hold the system in the same state while the algorithm iterates through each dimension using the same action.  The nature of RL MDPs is such that an action executed at one state would typically result in the system transitioning to a new state.  We are therefore forced to apply epsilon to all dimensions at once.  It is also often impossible to retrace the same trajectory since the initial conditions per episode might be different, or the policy might be updated in between episodes, leading to different actions at the same states.  This causes the output evaluations to see different gradients.

The former is expected and can be imagined as the local gradients at different locations in the policy reward surface.  The latter can be addressed by fixing a set of parameters over several trajectories, then updating the parameters only after estimating the output gradient for that prior parameter set.  The surface gradient can be estimated via regression using the cumulative discounted rewards from different trajectories.  Like in typical regression, the more data points provided, the better the estimate.

This data collection for each gradient regression must be balanced against execution time, and is a balancing act that is analogous to the stochastic vs mini-batch vs batch choices in SGD.  Among those techniques, SGD is often more robust in complex surfaces since it is more tolerant of local maxima.  With that in mind, I set the batch size to one sample (one pair of samples, see below), which works well.  However, I can get away with using only one datapoint because the initial conditions are very similar across episodes, so the regression estimate of the gradient remains relevant and near subsequent starting sensor values.

Finally, FD can estimate the gradient by either calculating the difference of the output with and without an epsilon change (forward/backward difference), or by taking a central difference approach where the output difference is due to a parameter set plus an epsilon change, and a parameter minus an epsilon change.  In the provided code, I use the central difference approach. -- PD

